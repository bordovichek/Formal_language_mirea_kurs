# Распознователь модельного языка
Итак, мною был выбран вариант 18 (231123), то есть мой язык имеет следующие правила:

__<операции_группы_отношения>__::= != | = = | < | <= | > | >=

__<операции_группы_сложения>__::= + | - | ||

__<операции_группы_умножения>__::= * | / | &&

__<унарная_операция>__::= !

__<структура программы>__ = {/ (<описание> | <оператор>) ( : | переход строки) /}
end

__<описание (объявление переменных)>__::= {<идентификатор> {, <идентификатор> } : <тип> ;}

__<тип данных (целый, с плавающей точкой, логический)>__::= % | ! | $

___Определение операторов___:

__<составной>__::= begin <оператор> { ; <оператор> } end

__<присваивания>__::= <идентификатор> := <выражение>

__<условный>__::= if «(»<выражение> «)» <оператор> [else <оператор>]

__<фиксированного_цикла>__::= for <присваивания> to <выражение>
[step <выражение>] <оператор> next

__<условного_цикла>__::= while «(»<выражение> «)» <оператор>

__<ввода>__::= readln идентификатор {, <идентификатор> }

__<вывода>__::= writeln <выражение> {, <выражение> }

__<определение комментария>__::= (* … *)

_А также все правила, общие для всех вариантов_

---

## Лексический анализатор

Задачи лексического анализатора: 
1) ___Группировка___ символов, составляющие исходную программу, в отдельные минимальные единицы текста, несущие смысловую нагрузку – ___лексемы___.
2) ___Замена___ идентификаторов, констант, ограничителей и служебных слов лексемами
3) ___Устранение___ из ее исходного представления несущественных пробелов и комментариев

В коде лексемы будут представляться __токенами__.
У токенов есть свои типы:
+ идентификатор(переменная)
+ число
+ оператор
+ тип данных
+ специальное слово
+ спецзначение End Of File для обозначения достижения конца файла
```
class TokenType:
    IDENTIFIER = 1
    NUMBER = 2
    OPERATOR = 3
    DATA_TYPE = 4
    SPECIAL = 5
    EOF = 6
   ```

Сам токен представляет собой объект с 3 атрибутами:
+ тип токена
+ значение
+ позиция в программе

И выводом всех своих атрибутов
```bazaar
class Token:
    def __init__(self, token_type, value, position):
        self.token_type = token_type
        self.value = value
        self.position = position

    def __repr__(self):
        return f'Токен({self.token_type}, {self.value}) на позиции {self.position}'
```

Заносим правила нашего языка в отдкльные списки и словари, где они хранят не только свои индекс, но и тип.
Прописываем регулярное выражение для поиска чисел. К слову, это __единственный__ момент, где можно использовать регулярки,
так как наша курсавая должна из себе представлять __конечный автомат__, который всегда итерируется __посимвольно__, поэтому и мы будем.

### Устройство работы анализатора

Анализатор посимвольно проходит по переданному тексту, ища непустные значения (то есть не пробелы, ведь их он пропускает).
Ищет совпадения для чисел, переменных, спецсимволов и операторов. Если ничего из этого не смог найти, то выводим ошибку со хначением ошибочной строки и ее позицией.
Пример:
```Ошибка при обработке 1a на позиции 94```(неправильное название переменной или объявление числа).
        
Для большинства вариантов __не понадобиться__ менять большое количество кода. Только __заменить/дополнить__ набор спецслов и логику проверок.

__В конце__ работы лексический анализатор выдаст __список  из кортежей__, в которых написан тип  токена и его порядковый номер.

---
## Синтаксический анализатор

Тут будет веселее, так как если лексическому анализатору были важны лишь символы и значение, а не то __как и где__ они написаны, то сейчас предстоит более сложная работа, так как теперь __это важно__.

Для облегчения работы, пропустим наш список из токенов через ```enumerate()``` чтобы мы могли не терять порядок обработки токенов.

Взглянем на объявление класса Syntax
```
class Syntax:
    def __init__(self, tokens: list[tuple[int, int]], n_str: int):
        self.tokens = tokens    # Список с токенами
        self.curr_pos = 0   # Текущая позиция
        self.token_ordering = list(enumerate(self.tokens))  # Пронумерованные токены
        self.n_str = n_str  # Количество строк
        self.current_line = -1  # Текущая строка
        self.program_vars = []  # Переменные программы
   ```

Мы начинаем итерироваться по __каждому значению__ в _token_ordering_ и обрабатывать каждую возможную ситуацию, в то же время отслеживая правильность структуры программы.
При нахождении ошибок будем вызывать __SyntaxError__ и по возможности описывать, что конкретно пошло не так.

За более конкретным описанием смотри комментарии в коде.

---

## Семантический анализатор

__Самый простой__ из анализаторов, но не менее важный. В его список задач входят:
+ Обработка описаний
  + проверка, что переменные программы описаны правильно и только один раз
+ Анализ выражений
  + проверка, описаны ли переменные, встречающиеся в выражениях, и соответствуют ли типы операндов друг другу и типу операции

Все эти задачи можно выполнить __на уровне синтаксического анализатора__, поэтому просто немного дополним его.
Например, проверку на повторное объявление можно выполнить в соответсвующей функции, просто перед переходом на следующий токен проверить, есть ли текущий в описанных переменных.
```bazaar
if self.token_ordering[self.curr_pos][1] in self.program_vars:
    raise SemanthicError("Объявление переменной второй раз")
else:
    self.program_vars.append(self.token_ordering[self.curr_pos][1])
```

Как вы заметили, я решил сделать свой тип ошибки __SemanthicError__ , что, разумеется, необязательно.

---
## Итоги

Итак, в результате работы всех трех анализаторов, мы должны __проверить на соответствие__ кода программы, составленного из варианта,
на правильность, __получив токены__ для каждого терминала и нетерминала или корректно __отработать ошибки__ на каждом из этапов.

Надеюсь, моя реализация помогла вам сделать собственный Лексический/Синтаксический/Семантический анализатор.